{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T13:35:26.656417Z",
     "iopub.status.busy": "2024-05-21T13:35:26.655866Z",
     "iopub.status.idle": "2024-05-21T13:35:26.658969Z",
     "shell.execute_reply": "2024-05-21T13:35:26.658585Z",
     "shell.execute_reply.started": "2024-05-21T13:35:26.656398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from diffusers import AutoPipelineForText2Image, DiffusionPipeline\n",
    "from diffusers_interpret import StableDiffusionPipelineDetExplainer\n",
    "\n",
    "from gill import layers\n",
    "\n",
    "from torch.cuda import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T13:35:27.617093Z",
     "iopub.status.busy": "2024-05-21T13:35:27.616556Z",
     "iopub.status.idle": "2024-05-21T13:35:27.619406Z",
     "shell.execute_reply": "2024-05-21T13:35:27.618922Z",
     "shell.execute_reply.started": "2024-05-21T13:35:27.617075Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/mnt/workspace/model'\n",
    "eva_id = 'zacbi2023/eva02'\n",
    "sd_id = 'AI-ModelScope/stable-diffusion-v1-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T13:35:28.238338Z",
     "iopub.status.busy": "2024-05-21T13:35:28.237801Z",
     "iopub.status.idle": "2024-05-21T13:35:31.714889Z",
     "shell.execute_reply": "2024-05-21T13:35:31.714352Z",
     "shell.execute_reply.started": "2024-05-21T13:35:28.238310Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.net.blocks.0.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.0.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.1.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.1.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.10.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.10.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.11.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.11.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.12.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.12.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.12.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.12.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.13.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.13.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.13.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.13.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.14.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.14.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.14.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.14.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.15.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.15.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.15.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.15.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.16.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.16.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.16.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.16.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.17.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.17.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.17.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.17.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.18.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.18.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.18.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.18.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.19.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.19.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.19.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.19.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.2.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.2.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.20.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.20.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.20.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.20.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.21.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.21.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.21.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.21.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.22.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.22.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.22.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.22.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.23.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.23.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.23.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.23.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.3.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.3.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.4.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.4.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.5.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.5.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.6.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.6.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.7.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.7.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.8.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.8.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.9.attn.qkv.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.9.attn.{rel_pos_h, rel_pos_w}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.net.blocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mpixel_mean\u001b[0m\n",
      "  \u001b[35mpixel_std\u001b[0m\n",
      "  \u001b[35mbackbone.net.rope_win.{freqs_cos, freqs_sin}\u001b[0m\n",
      "  \u001b[35mbackbone.net.rope_glb.{freqs_cos, freqs_sin}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.0.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.1.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.2.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.3.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.4.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.5.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.6.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.7.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.8.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.9.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.10.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.11.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.12.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.13.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.14.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.15.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.16.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.17.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.18.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.19.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.20.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.21.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.22.mlp.w3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.attn.q_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.attn.k_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.attn.v_proj.weight\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.mlp.w1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.mlp.w2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.mlp.ffn_ln.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.net.blocks.23.mlp.w3.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneralizedRCNN(\n",
       "  (backbone): SimpleFeaturePyramid(\n",
       "    (simfp_2): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): Conv2d(\n",
       "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (5): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (simfp_3): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): Conv2d(\n",
       "        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (2): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (simfp_4): Sequential(\n",
       "      (0): Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (1): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (simfp_5): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(\n",
       "        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (2): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (net): ViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.104)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.117)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.130)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.143)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.157)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.170)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.183)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.196)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.209)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.222)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.235)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.248)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.261)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.274)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.287)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.300)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (top_block): LastLevelMaxPool()\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Sequential(\n",
       "        (conv0): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (conv1): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): CascadeROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): ModuleList(\n",
       "      (0-2): 3 x FastRCNNConvFCHead(\n",
       "        (conv1): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): LayerNorm()\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (conv2): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): LayerNorm()\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (conv3): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): LayerNorm()\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (conv4): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (norm): LayerNorm()\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc_relu1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (box_predictor): ModuleList(\n",
       "      (0-2): 3 x FastRCNNOutputLayers(\n",
       "        (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_head): MaskRCNNConvUpsampleHead(\n",
       "      (mask_fcn1): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (mask_fcn2): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (mask_fcn3): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (mask_fcn4): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): LayerNorm()\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (deconv_relu): ReLU()\n",
       "      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare eva\n",
    "eva_base_path = path.join(model_path, eva_id)\n",
    "eva_coco_config_rpath = 'projects/ViTDet/configs/eva2_o365_to_coco/eva2_o365_to_coco_cascade_mask_rcnn_vitdet_l_8attn_1536_lrd0p8.py'\n",
    "eva_config_path = path.join(eva_base_path, eva_coco_config_rpath)\n",
    "\n",
    "# replace with your eva02 weights path\n",
    "eva_coco_weights_rpth = 'checkpoints/eva02_L_coco_seg_sys_o365.pth'\n",
    "eva_weights_path = path.join(eva_base_path, eva_coco_weights_rpth)\n",
    "\n",
    "custum_cfg = ['MODEL.RETINANET.SCORE_THRESH_TEST', 0.5,\n",
    "                'MODEL.ROI_HEADS.SCORE_THRESH_TEST', 0.5,\n",
    "                'MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH', 0.5,\n",
    "                'DATASETS.TEST', [],\n",
    "                'MODEL.WEIGHTS', eva_weights_path]\n",
    "eva_cfg = LazyConfig.load(eva_config_path)\n",
    "LazyConfig.apply_overrides(\n",
    "    eva_cfg, [f\"{key}={value}\" for key, value in zip(custum_cfg[::2], custum_cfg[1::2])])\n",
    "\n",
    "device = 'cuda'\n",
    "eva = instantiate(eva_cfg.model).to(device)\n",
    "DetectionCheckpointer(eva).load(eva_weights_path)\n",
    "eva.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T13:35:38.051602Z",
     "iopub.status.busy": "2024-05-21T13:35:38.051273Z",
     "iopub.status.idle": "2024-05-21T13:35:40.748164Z",
     "shell.execute_reply": "2024-05-21T13:35:40.747470Z",
     "shell.execute_reply.started": "2024-05-21T13:35:38.051583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  71%|  | 5/7 [00:02<00:00,  2.81it/s]/opt/conda/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|| 7/7 [00:02<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "torch_dtype=torch.bfloat16\n",
    "sd_pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    path.join(model_path, sd_id), torch_dtype=torch_dtype).to(device)\n",
    "explainer = StableDiffusionPipelineDetExplainer(pipe=sd_pipe, det_model=eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T13:35:42.449029Z",
     "iopub.status.busy": "2024-05-21T13:35:42.448719Z",
     "iopub.status.idle": "2024-05-21T13:35:42.728723Z",
     "shell.execute_reply": "2024-05-21T13:35:42.728112Z",
     "shell.execute_reply.started": "2024-05-21T13:35:42.449010Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenTextHiddenFcs(\n",
       "  (gen_text_hidden_fcs): ModuleList(\n",
       "    (0): TextFcLayer(\n",
       "      (fc): Linear(in_features=4096, out_features=512, bias=True)\n",
       "      (tfm): Transformer(\n",
       "        (encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x TransformerEncoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.0, inplace=False)\n",
       "              (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): TransformerDecoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x TransformerDecoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.0, inplace=False)\n",
       "              (dropout2): Dropout(p=0.0, inplace=False)\n",
       "              (dropout3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (model): Linear(in_features=512, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm(batch_size, seq_len, hidden_dim)\n",
    "raw_emb = torch.load('/mnt/workspace/data/tensor/raw_emb_tensor_cat_1.pt').to(torch_dtype)\n",
    "raw_emb.requires_grad_(True)\n",
    "# embedding img0-imge8\n",
    "gen_prefix_embs = torch.load('/mnt/workspace/data/tensor/gen_prefix_embs_tensor_cat_1.pt').to(torch_dtype)\n",
    "gen_prefix_embs.requires_grad_(True)\n",
    "\n",
    "# gill_mapper: linear + Transformer + linear\n",
    "gen_text_hidden_fcs = layers.GenTextHiddenFcs()\n",
    "gill_state_dict = torch.load('/mnt/workspace/github/gill/checkpoints/gill_opt/pretrained_ckpt.pth.tar')\n",
    "\n",
    "gen_text_hidden_fcs_state_dict = {}\n",
    "for key, val in gill_state_dict['state_dict'].items():\n",
    "    if 'gen_text_hidden_fcs' in key:\n",
    "        prefix = 'gen_text_hidden_fcs' + key.split('gen_text_hidden_fcs')[1]\n",
    "        gen_text_hidden_fcs_state_dict[prefix] = val\n",
    "gen_text_hidden_fcs.load_state_dict(gen_text_hidden_fcs_state_dict)\n",
    "gen_text_hidden_fcs.cuda()\n",
    "gen_text_hidden_fcs.to(torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T13:35:47.299248Z",
     "iopub.status.busy": "2024-05-21T13:35:47.298949Z",
     "iopub.status.idle": "2024-05-21T13:35:49.193374Z",
     "shell.execute_reply": "2024-05-21T13:35:49.192877Z",
     "shell.execute_reply.started": "2024-05-21T13:35:47.299230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_emb = gen_text_hidden_fcs.gen_text_hidden_fcs[0](raw_emb, gen_prefix_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T12:28:56.243903Z",
     "iopub.status.busy": "2024-05-20T12:28:56.243559Z",
     "iopub.status.idle": "2024-05-20T12:29:08.725732Z",
     "shell.execute_reply": "2024-05-20T12:29:08.724747Z",
     "shell.execute_reply.started": "2024-05-20T12:28:56.243880Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/github/diffusers-interpret/src/diffusers_interpret/explainers/stable_diffusion.py:147: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  latents_shape = (batch_size, self.pipe.unet.in_channels, height // 8, width // 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72e59616c5e47409053189a3d8fdb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token attributions... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/snpa/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608851799/work/aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/mnt/workspace/github/EVA/EVA-02/det/detectron2/layers/wrappers.py:113: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608851799/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  x = F.conv2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    output = explainer(\n",
    "        prompt_embeds=gen_emb,\n",
    "        num_inference_steps=50,\n",
    "        target_cls_id=15,\n",
    "        raw_embeds=raw_emb,\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0.7846115),\n",
       " ('1', 1.3332276),\n",
       " ('2', 1.826821),\n",
       " ('3', 2.8268588),\n",
       " ('4', 0.3671959),\n",
       " ('5', 1.0050043),\n",
       " ('6', 1.2384505),\n",
       " ('7', 0.6856177)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.token_attributions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
